{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "exotic-fault",
   "metadata": {},
   "source": [
    "# WEEK 3 : MACHINE LEARNING WORKFLOW & DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-excellence",
   "metadata": {},
   "source": [
    "Sumber : \n",
    "- https://www.youtube.com/watch?v=smNnhEd26Ek&ab_channel=IndonesiaBelajar\n",
    "- https://www.youtube.com/watch?v=tiREcHrtDLo&ab_channel=IndonesiaBelajar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-treatment",
   "metadata": {},
   "source": [
    "## WORKFLOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-discovery",
   "metadata": {},
   "source": [
    "### Load Sample Dataset Irit Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "wrong-crisis",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-kentucky",
   "metadata": {},
   "source": [
    "### Splitting Dataset : Training & Testing Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emerging-stanley",
   "metadata": {},
   "source": [
    "Setelah melakukan load dataset, langkah selanjutnya adalah membagi dataset / splitting dataset ini menjadi 2 bagian yang terdiri dari Training Set dan Testing Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adolescent-claim",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                   y,\n",
    "                                                   test_size=0.4,\n",
    "                                                   random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-fiction",
   "metadata": {},
   "source": [
    "Disini testing set memiliki proporsi 40% sedangkan untuk training setnya adalah 60%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-reunion",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-possible",
   "metadata": {},
   "source": [
    "- Pada Scikit Learn, model machine learning dibentuk dari class yang dikenal dengan istilah estimator\n",
    "- Setiap estimator akan mengimplementasikan dua method utama, yaitu method fit() dan predict()\n",
    "- Method fit() biasanya digunakan untuk melakukan training model\n",
    "- Method predict() digunakan untuk melakukan estimasi/prediksi dengan memanfaatkan trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-split",
   "metadata": {},
   "source": [
    "Pada tahap ini, saya menggunakan KNN sebagai machine learning model. Model ini akan saya training dengan memanfaatkan training set yang sudah dipersiapkan sebelumnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "governmental-marking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-experience",
   "metadata": {},
   "source": [
    "### Evaluasi Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-dispute",
   "metadata": {},
   "source": [
    "Pada tahap sebelumnya yaitu splitting dataset, saya sudah membagi dataset menjadi 2 bagian yaitu training set dan testing set. Training set digunakan untuk melakukan proses training model, sedangkan testing set digunakan untuk melakukan proses evaluasi/testing performa dari model yang sudah ditraining sebelumnya."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chicken-creator",
   "metadata": {},
   "source": [
    "Cara untuk melakukan evaluasi model adalah :\n",
    "- Import accuracy_score\n",
    "- Langkah selanjutnya, saya akan melakukan prediksi terhadap nilai features yang ada di dalam testing dataset. Kemudian hasil prediksinya ditampung ke dalam variable y_predict\n",
    "- Pada tahap ini, kita akan membandingkan nilai target yang terdapat pada variable y_test, dan dibandingkan dengan nilai prediksi yang ditampung dalam variable y_predict. \n",
    "- Sebenarnya proses evalusi ini terdiri dari berbagai macam cara, tetapi saat ini saya menggunakan accuracy score. Kemudian nilai akurasinya saya tampung ke dalam variable accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "foreign-lindsay",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.9833333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_predict = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "print(f'Accuracy:{accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charming-audience",
   "metadata": {},
   "source": [
    "### Pemanfaatan Trained Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-values",
   "metadata": {},
   "source": [
    "Model yang telah ditraining akan digunakan untuk melakukan prediksi terhadap data baru. Pada data baru hanya terdapat nilai features dan tidak memiliki nilai target. Pada tahap ini, saya akan menggunakan model yang sudah ditraining sebelumnya untuk memprediksi nilai target dari data baru yang berisi sekumpulan nilai features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-document",
   "metadata": {},
   "source": [
    "Pertama, saya bentuk dataset yang baru yang ditampung dalam variable data_baru yang terdiri dari 2 baris dan setiap barisnya memiliki 4 nilai features.\n",
    "Hasil output dari prediksi yang telah dilakukan adalah array([1,2]), ini berarti pada instance 1 diprediksi memiliki nilai target 1 dan untuk instance 2 diprediksi memiliki nilai target 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "assisted-somewhere",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_baru = [[5, 5, 3, 2],\n",
    "            [2, 4, 3, 5 ]]\n",
    "preds = model.predict(data_baru)\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-pendant",
   "metadata": {},
   "source": [
    "#### Proses untuk Memanggil Target Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "resident-network",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil Prediksi: ['versicolor', 'virginica']\n"
     ]
    }
   ],
   "source": [
    "pred_species = [iris.target_names[p] for p in preds]\n",
    "print(f'Hasil Prediksi: {pred_species}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-elephant",
   "metadata": {},
   "source": [
    "### Dump & Load Trained Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-differential",
   "metadata": {},
   "source": [
    "#### Dumping Model Machine Leaning menjadi file joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-present",
   "metadata": {},
   "source": [
    "Pada tahap ini, saya akan melakukan proses joblib. \n",
    "Pertama-tama, saya import terlebih dahulu joblibnya, kemudian saya melakukan proses dump pada model yang sudah ditraining sebelumnya ke dalam suatu file joblib yaitu iris_classifier_knn.joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "allied-memorabilia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['iris_classifier_knn.joblib']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(model, 'iris_classifier_knn.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-corruption",
   "metadata": {},
   "source": [
    "#### Loading Model Machine Learning dari file joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-little",
   "metadata": {},
   "source": [
    "Untuk melakukan deployment machine learning model ini, maka hal yang perlu dilakukan adalah menempatkan file joblib yaitu iris_classifier_knn.joblib ke dalam production server. Pada production server, kita akan load file joblib ini untuk menjadi machine learning model yang siap digunakan. Setelah diload, maka production_model ini bisa digunakan untuk memprediksi data-data baru yang ditemui di production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "limiting-missouri",
   "metadata": {},
   "outputs": [],
   "source": [
    "production_model = joblib.load('iris_classifier_knn.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-vector",
   "metadata": {},
   "source": [
    "## Data Preprocessing dengan Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-permission",
   "metadata": {},
   "source": [
    "### Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-artist",
   "metadata": {},
   "source": [
    "Pada tahap ini, hal yang perlu dilakukan adalah mengimport numpy karena sample datanya akan digenerate sebagai numpy array. Kemudian import module preprocessing dari sklearn. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-interim",
   "metadata": {},
   "source": [
    "Dapat dilihat bahwa, sample data dami ini memiliki 4 instance dan masing-masing instance memiliki 3 nilai features. Dataset ini ditampung kedalam variable yaitu sample_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "material-cleveland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.1, -1.9,  5.5],\n",
       "       [-1.5,  2.4,  3.5],\n",
       "       [ 0.5, -7.9,  5.6],\n",
       "       [ 5.9,  2.3, -5.8]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "sample_data = np.array([[2.1, -1.9, 5.5],\n",
    "                       [-1.5, 2.4, 3.5],\n",
    "                       [0.5, -7.9, 5.6],\n",
    "                       [5.9, 2.3, -5.8]])\n",
    "sample_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-links",
   "metadata": {},
   "source": [
    "Untuk melihat dimensi dari sample datanya dapat menggunakan method shape. Dapat dilihat dari outputnya, bahwa nilai baris/instancenya adalah 4 dan nilai features/kolomnya adalah 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "revolutionary-royalty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-placement",
   "metadata": {},
   "source": [
    "### Binarisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-think",
   "metadata": {},
   "source": [
    "Binarisation ini bertujuan untuk menghasilkan suatu data yang terdiri dari 2 nilai numerik saja, yaitu 0 dan 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "featured-lightweight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.1, -1.9,  5.5],\n",
       "       [-1.5,  2.4,  3.5],\n",
       "       [ 0.5, -7.9,  5.6],\n",
       "       [ 5.9,  2.3, -5.8]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-programming",
   "metadata": {},
   "source": [
    "Pada tahap binarisation terdapat method threshold yang artinya setiap nilai yang lebih kecil atau sama dengan 0.5 akan dikonversikan menjadi nilai 0, sedangkan setiap nilai yang lebih besar 0.5 akan dikonversikan menjadi nilai 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "young-precipitation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 1.],\n",
       "       [0., 1., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 1., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = preprocessing.Binarizer(threshold=0.5)\n",
    "binarised_data = preprocessor.transform(sample_data)\n",
    "binarised_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-preview",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-temperature",
   "metadata": {},
   "source": [
    "Teknik scaling bertujuan untuk menghasilkan suatu data numerik yang berada dalam rentang skala tertentu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "continuous-graphic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.1, -1.9,  5.5],\n",
       "       [-1.5,  2.4,  3.5],\n",
       "       [ 0.5, -7.9,  5.6],\n",
       "       [ 5.9,  2.3, -5.8]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-refund",
   "metadata": {},
   "source": [
    "Pada tahap ini, saya akan menggunakan teknik MinMaxScaler dimana terdapat parameter feature_range yang membutuhkan 2 data bertipe tuple. Disini saya memberikan nilai 0 sebagai nilai terkecil dari skala yang baru dan nilai 1 sebagai nilai terbesar dari skala yang baru. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-article",
   "metadata": {},
   "source": [
    "Kemudian object scaler yang ditampung dalam variable preprocessor ini akan saya fit terhadap sample data yang saya miliki. Kemudian setelah dilakukan proses fit, maka scaler ini bisa digunakan untuk melakukan proses tranformasi data terhadap sample data. Hasil dari transform data ini akan saya tampung dalam variable scaled_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "outdoor-crossing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.48648649, 0.58252427, 0.99122807],\n",
       "       [0.        , 1.        , 0.81578947],\n",
       "       [0.27027027, 0.        , 1.        ],\n",
       "       [1.        , 0.99029126, 0.        ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "preprocessor.fit(sample_data)\n",
    "scaled_data = preprocessor.transform(sample_data)\n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-member",
   "metadata": {},
   "source": [
    "Pada proses dibawah ini menjelaskan bahwa proses fit dan transform dikenakan pada data yang sama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "sublime-batch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.48648649, 0.58252427, 0.99122807],\n",
       "       [0.        , 1.        , 0.81578947],\n",
       "       [0.27027027, 0.        , 1.        ],\n",
       "       [1.        , 0.99029126, 0.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_data = preprocessor.fit_transform(sample_data)\n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-juvenile",
   "metadata": {},
   "source": [
    "### L1 Normalisation : Least Absolute Deviations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-mitchell",
   "metadata": {},
   "source": [
    "Teknik Normalisation ini bertujuan untuk melakukan normalisasi terhadap data numerik yang kita miliki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "super-witch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.1, -1.9,  5.5],\n",
       "       [-1.5,  2.4,  3.5],\n",
       "       [ 0.5, -7.9,  5.6],\n",
       "       [ 5.9,  2.3, -5.8]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-keeping",
   "metadata": {},
   "source": [
    "Pemanggilan data preprocessing normalize ini akan menghasilkan data yang sudah ternormalisasi yang tertampung pada variable l1_normalised_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "varying-revelation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.22105263, -0.2       ,  0.57894737],\n",
       "       [-0.2027027 ,  0.32432432,  0.47297297],\n",
       "       [ 0.03571429, -0.56428571,  0.4       ],\n",
       "       [ 0.42142857,  0.16428571, -0.41428571]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_normalised_data = preprocessing.normalize(sample_data,norm='l1')\n",
    "l1_normalised_data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-research",
   "metadata": {},
   "source": [
    "### L2 Normalisation: Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "black-windows",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.1, -1.9,  5.5],\n",
       "       [-1.5,  2.4,  3.5],\n",
       "       [ 0.5, -7.9,  5.6],\n",
       "       [ 5.9,  2.3, -5.8]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-knight",
   "metadata": {},
   "source": [
    "Perbedaan normalisation Least Absolute Deviations dengan Least Squares adalah pada parameter norm. Pada least absolute deviations, parameter norm berisi 'l1' sedangkan pada least squares, parameter norm berisi 'l2'.\n",
    "- l1 : Least Absolute Deviations\n",
    "- l2 : Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "atlantic-lodge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.33946114, -0.30713151,  0.88906489],\n",
       "       [-0.33325106,  0.53320169,  0.7775858 ],\n",
       "       [ 0.05156558, -0.81473612,  0.57753446],\n",
       "       [ 0.68706914,  0.26784051, -0.6754239 ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_normalised = preprocessing.normalize(sample_data, norm='l2')\n",
    "l2_normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-account",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
