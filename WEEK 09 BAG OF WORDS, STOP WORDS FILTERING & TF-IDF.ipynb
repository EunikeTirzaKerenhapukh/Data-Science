{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6f33ad3",
   "metadata": {},
   "source": [
    "# BAG OF WORDS & STOP WORD FILTERING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699db1bb",
   "metadata": {},
   "source": [
    "Sumber :\n",
    "- https://www.youtube.com/watch?v=U30sF4m0bd0&ab_channel=IndonesiaBelajar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e27a7a",
   "metadata": {},
   "source": [
    "## Bag of Words model sebagai representasi text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f181400d",
   "metadata": {},
   "source": [
    "Menurut Wikipedia, Bag of Words menyerdehanakan representasi text sebagai sekumpulan kata serta mengabaikan grammar dan posisi tiap kata pada kalimat. Text dikonveri menjadi lowercase/huruf kecil dan tanda baca akan diabaikan. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b98294b",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edac062",
   "metadata": {},
   "source": [
    "Pertama-tama, siapkan dataset text terlebih dahulu yang berisi kalimat-kalimat pendek. Dataset text ini biasa disebut dengan corpus dan dalam dataset yang saya buat memiliki 3 kalimat pendek di dalamnya. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d70d495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Linux has been around since the mid-1990s.',\n",
       " 'Linux distributions include the Linex kernel',\n",
       " 'Linux is one of the most prominent open-source software']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "    'Linux has been around since the mid-1990s.',\n",
    "    'Linux distributions include the Linex kernel',\n",
    "    'Linux is one of the most prominent open-source software'\n",
    "]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68a7791",
   "metadata": {},
   "source": [
    "## Bags of Words model dengan CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7d7cf2",
   "metadata": {},
   "source": [
    "Bags of words model dapat diterapkan dengan memanfaatkan CountVextorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e920d84",
   "metadata": {},
   "source": [
    "Langkah-langkah:\n",
    "- Mengimport modulnya yaitu CountVectorizer\n",
    "- Membuat objek dari class CountVectorizer dengan memanggil CountVectorizer() dan ditampung ke dalam variable 'vectorizer'\n",
    "- Kemudian objek 'vectorizer' akan digunakan untuk melakukan metode fit_transform terhadap dataset corpus. Lalu hasilnya akan dikonversikan ke dalam array, oleh karena itu menggunakan method todense() dan hasilnya akan dimasukkan ke dalam variable 'vectorized_X'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f523b87e",
   "metadata": {},
   "source": [
    "Method todense() berguna untuk mengubah hasil fit_transform menjadi sebuah array 2 dimensi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b179dbb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1],\n",
       "        [0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1]],\n",
       "       dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorized_X = vectorizer.fit_transform(corpus).todense()\n",
    "vectorized_X "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23494c4",
   "metadata": {},
   "source": [
    "#### Karena kita menggunakan teknik Bag of Words, maka kata-kata hasil output dibawah ini adalah sekumpulan kata yang berada di dalam keranjang/bags tersebut. Kemudian kata-kata dibawah ini sudah diurutkan secara alfabetical dan menjadi lowercase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044b1749",
   "metadata": {},
   "source": [
    "#### Jika diperhatikan dengan matrix yang diatas, angka pertama adalah 1 yang artinya kata '1990s' ada dalam kalimat tersebut. Apabila angkanya 0 maka kata tersebut tidak ada di dalam kalimat tersebut.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ce06703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1990s',\n",
       " 'around',\n",
       " 'been',\n",
       " 'distributions',\n",
       " 'has',\n",
       " 'include',\n",
       " 'is',\n",
       " 'kernel',\n",
       " 'linex',\n",
       " 'linux',\n",
       " 'mid',\n",
       " 'most',\n",
       " 'of',\n",
       " 'one',\n",
       " 'open',\n",
       " 'prominent',\n",
       " 'since',\n",
       " 'software',\n",
       " 'source',\n",
       " 'the']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b6f197",
   "metadata": {},
   "source": [
    "### Euclidean Distance untuk mengukur kedekatan/jarak antar dokumen (vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28860dcb",
   "metadata": {},
   "source": [
    "Dengan representasi Bag of Words suatu algoritma machine learning dapat lebih mudah mengukur kedekatan/kemiripan antar dokumen. \n",
    "Pada tahap ini saya akan menggunakan eculidean distance untuk mengukur kedekatan antar dokumen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24318aea",
   "metadata": {},
   "source": [
    "Langkah-langkah:\n",
    "- import modul euclidean_distances\n",
    "- mengukur jarak antar kalimatnya (kalimat pertama akan diukur kedekatannya dengan kalimat kedua dan kalimat pertama akan diukur kedekatannya dengan kalimat ketiga, kalimat kedua akan diukur kedekatannya dengan kalimat ketiganya). \n",
    "- Untuk mengukur kedekatannya kita akan menggunakan for loop statement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9143270d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jarak dokumen 1 dan 2:[[3.16227766]]\n",
      "Jarak dokumen 1 dan 3:[[3.74165739]]\n",
      "Jarak dokumen 2 dan 3:[[3.46410162]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "for i in range(len(vectorized_X)):\n",
    "    for j in range(i,len(vectorized_X)):\n",
    "        if i == j:\n",
    "            continue\n",
    "        jarak = euclidean_distances(vectorized_X[i],vectorized_X[j])\n",
    "        print(f'Jarak dokumen {i+1} dan {j+1}:{jarak}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d08a10",
   "metadata": {},
   "source": [
    "Dapat dilihat hasil output diatas bahwa jarak dokumen 1 dan 2 paling tinggi diantara yang lainnya. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e9ed76",
   "metadata": {},
   "source": [
    "## Stop Word FIltering pada text "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b989eb2",
   "metadata": {},
   "source": [
    "Menurut Wikipedia, Stop Word Filtering menyederhanakan representasi text dengan mengabaikan beberapa kata seperti determiners (the, a, an), auxiliary verbs (do, will, be) dan preposition (on, in, at)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53b8be8",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a000279c",
   "metadata": {},
   "source": [
    "Kita akan menggunakan dataset yang telah dipakai sebelumnya yaitu dataset corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a104b3",
   "metadata": {},
   "source": [
    "Dapat dilihat bahwa dalam dataset corpus ini terdapat kata-kata seperti has, the, been, is, dan of. Kata-kata ini akan menjadi stop word/kata-kata yang akan kita abaikan. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "27e9dff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Linux has been around since the mid-1990s.',\n",
       " 'Linux distributions include the Linex kernel',\n",
       " 'Linux is one of the most prominent open-source software']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d154aa3a",
   "metadata": {},
   "source": [
    "## Stop Word Filtering dengan CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c84405",
   "metadata": {},
   "source": [
    "Stop Word Filtering juga dapat diterapkan dengan memanfaatkan CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87de237b",
   "metadata": {},
   "source": [
    "Langkah-langkah:\n",
    "- Mengimport module CountVectorizer\n",
    "- Kita akan membentuk objek dari class CountVectorizer dengan menggunakan parameter stop_words yang berisi 'english', karena kita akan menggunakan stop wordsnya dalam bahasa inggris. Objek ini akan ditampung ke dalam variable 'vectorizer'\n",
    "- Kemudian 'vectorizer' ini akan digunakan untuk melakukan fit_transform terhadap dataset corpus dan dikonversikan ke dalam array 2 dimensi dengan menggunakan todense(). Hasilnya akan ditampung ke dalamv variable 'vectorized_X'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0da08bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
       "        [0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "vectorized_X = vectorizer.fit_transform(corpus).todense()\n",
    "vectorized_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0843d528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1990s',\n",
       " 'distributions',\n",
       " 'include',\n",
       " 'kernel',\n",
       " 'linex',\n",
       " 'linux',\n",
       " 'mid',\n",
       " 'open',\n",
       " 'prominent',\n",
       " 'software',\n",
       " 'source']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd635ec",
   "metadata": {},
   "source": [
    "Dapat dilihat dari hasil output di atas bahwa ada beberapa kata yang hilang berarti dapat disimpulkan bahwa kita berhasil untuk menggunakan metode stop words. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1863e5c",
   "metadata": {},
   "source": [
    "# TF-IDF Term Frequency - Inverse Document Frequency "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfcd655",
   "metadata": {},
   "source": [
    "Sumber :\n",
    "- https://www.youtube.com/watch?v=f0a1XXmaQp8&ab_channel=IndonesiaBelajar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03535288",
   "metadata": {},
   "source": [
    "TF-IDF (Term Frequency - Inverse Document Frequency) merupakan salah satu metode statistik yang digunakan untuk mengukur seberapa penting suatu kata terhadap suatu dokumen tertentu dari sekumpulan dokumen atau corpus. \n",
    "\n",
    "Referensi :\n",
    "- https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
    "- https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3090b2",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c152a7",
   "metadata": {},
   "source": [
    "Pertama-tama, siapkan dataset corpus yang berisi 5 kalimat pendek. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fe965f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the house had a tiny little mouse',\n",
       " 'the cat saw the mouse',\n",
       " 'the mouse ran away from the house',\n",
       " 'the cat finally ate the mouse',\n",
       " 'the end of the mouse story']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\n",
    "    'the house had a tiny little mouse',\n",
    "    'the cat saw the mouse',\n",
    "    'the mouse ran away from the house',\n",
    "    'the cat finally ate the mouse',\n",
    "    'the end of the mouse story'\n",
    "]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b64f8e1",
   "metadata": {},
   "source": [
    "## TF-IDF Weights dengan TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73140c60",
   "metadata": {},
   "source": [
    "Langkah-langkah:\n",
    "- Import module TfidfVectorizer\n",
    "- Membuat objek dari Class TfidfVectorizer dengan parameter stop_words yang berisi 'english'. Kemudian objek ini akan ditampung ke dalam variable 'vectorizer'\n",
    "- Kita akan menggunakan 'vectorizer' untuk melakukan fit_transform terhadap dataset corpus. Lalu hasil fit_transform akan dimasukkan ke dalam variable 'response'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a9cf85d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7)\t0.2808823162882302\n",
      "  (0, 6)\t0.5894630806320427\n",
      "  (0, 11)\t0.5894630806320427\n",
      "  (0, 5)\t0.47557510189256375\n",
      "  (1, 9)\t0.7297183669435993\n",
      "  (1, 2)\t0.5887321837696324\n",
      "  (1, 7)\t0.3477147117091919\n",
      "  (2, 1)\t0.5894630806320427\n",
      "  (2, 8)\t0.5894630806320427\n",
      "  (2, 7)\t0.2808823162882302\n",
      "  (2, 5)\t0.47557510189256375\n",
      "  (3, 0)\t0.5894630806320427\n",
      "  (3, 4)\t0.5894630806320427\n",
      "  (3, 2)\t0.47557510189256375\n",
      "  (3, 7)\t0.2808823162882302\n",
      "  (4, 10)\t0.6700917930430479\n",
      "  (4, 3)\t0.6700917930430479\n",
      "  (4, 7)\t0.3193023297639811\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "response = vectorizer.fit_transform(corpus)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af23e8c",
   "metadata": {},
   "source": [
    "Dari hasil output diatas :\n",
    "- Angka yang paling kiri merepresentasikan index dari corpus. \n",
    "  Index 0 merepresentasikan kalimat pertama dari corpus\n",
    "  Index 1 merepresentasikan kalimat kedua dari corpus\n",
    "  Index 2 merepresentasikan kalimat ketiga dari corpus, dst\n",
    "- Angka kedua dari paling kiri merepresentasikan index dari features name yang dihasilkan dari Bag of Words. Contohnya adalah (0,7) berarti pada kalimat pertama mengandung kata 'mouse' (dilihat dari features_name).\n",
    "- Sekumpulan angka di paling kanan merepresentasikan bobot dari tiap TF-IDF dari hasil kalkulasi TF-IDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cb5914c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ate',\n",
       " 'away',\n",
       " 'cat',\n",
       " 'end',\n",
       " 'finally',\n",
       " 'house',\n",
       " 'little',\n",
       " 'mouse',\n",
       " 'ran',\n",
       " 'saw',\n",
       " 'story',\n",
       " 'tiny']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4437964a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.4755751 , 0.58946308, 0.28088232, 0.        , 0.        ,\n",
       "         0.        , 0.58946308],\n",
       "        [0.        , 0.        , 0.58873218, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.34771471, 0.        , 0.72971837,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.58946308, 0.        , 0.        , 0.        ,\n",
       "         0.4755751 , 0.        , 0.28088232, 0.58946308, 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.58946308, 0.        , 0.4755751 , 0.        , 0.58946308,\n",
       "         0.        , 0.        , 0.28088232, 0.        , 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.67009179, 0.        ,\n",
       "         0.        , 0.        , 0.31930233, 0.        , 0.        ,\n",
       "         0.67009179, 0.        ]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82f50a0",
   "metadata": {},
   "source": [
    "Dapat dilihat melalui output dibawah bahwa kata 'cat' terdapat dalam dokumen kedua(D2) dan dalam dokumen keempat(D4). Hanya saja kata 'cat' ini memiliki bobot yang lebih tinggi pada D2 dibandingkan dengan D4. Semakin tinggi bobot suatu kata terhadap suatu dokumen maka kata tersebut semakin layak untuk digunakan sebagai keywords terhadap dokumen tersebut. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "27ad6e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>D5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ate</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.589463</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>away</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.589463</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.588732</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.475575</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.670092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finally</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.589463</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>house</th>\n",
       "      <td>0.475575</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.475575</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>little</th>\n",
       "      <td>0.589463</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mouse</th>\n",
       "      <td>0.280882</td>\n",
       "      <td>0.347715</td>\n",
       "      <td>0.280882</td>\n",
       "      <td>0.280882</td>\n",
       "      <td>0.319302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ran</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.589463</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saw</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.729718</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>story</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.670092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tiny</th>\n",
       "      <td>0.589463</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               D1        D2        D3        D4        D5\n",
       "ate      0.000000  0.000000  0.000000  0.589463  0.000000\n",
       "away     0.000000  0.000000  0.589463  0.000000  0.000000\n",
       "cat      0.000000  0.588732  0.000000  0.475575  0.000000\n",
       "end      0.000000  0.000000  0.000000  0.000000  0.670092\n",
       "finally  0.000000  0.000000  0.000000  0.589463  0.000000\n",
       "house    0.475575  0.000000  0.475575  0.000000  0.000000\n",
       "little   0.589463  0.000000  0.000000  0.000000  0.000000\n",
       "mouse    0.280882  0.347715  0.280882  0.280882  0.319302\n",
       "ran      0.000000  0.000000  0.589463  0.000000  0.000000\n",
       "saw      0.000000  0.729718  0.000000  0.000000  0.000000\n",
       "story    0.000000  0.000000  0.000000  0.000000  0.670092\n",
       "tiny     0.589463  0.000000  0.000000  0.000000  0.000000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(response.todense().T,\n",
    "                 index=vectorizer.get_feature_names(),\n",
    "                 columns=[f'D{i+1}' for i in range(len(corpus))])\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
